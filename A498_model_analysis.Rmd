---
title: "A498 Cell Line Model Analysis"
output:
  html_document:
    df_print: paged
    #self_contained: no
---

## Intro

This notebook includes the ensemble model analysis performed on the models 
generated by the `Gitsbe` module when running the DrugLogics computational 
pipeline for finding synergistic drug combinations (drug pairs). All these 
models were trained towards a specific steady state signaling pattern that was
derived based on input data (gene expression, CNV) for the `A498` cell line, the 
use of the [PARADIGM](https://doi.org/10.1093/bioinformatics/btq182) software and 
a topology that was build for simulating a cancer cell fate decision network.
The input for the simulations and the output data are in the `cell-lines-2500` 
directory (the 2500 number denotes the number of simulations executed). The 
analysis will be presented step by step in the sections below.

The R version used for this analysis is: **3.4.4 (Someone to Lean On)**.

## Prerequisites

Firstly, we load the required libraries (you need to install them if you don't
have them):
```{r Load libraries, message = FALSE}
# require(RColorBrewer)
# require(gplots)
library("rje") # version: 1.9
library("igraph") # version: 1.2.2, see (Csardi, 2006)
```
and the relevant helper functions:
```{r Helper functions}
# Set the working directory to the gitsbe-model-analysis folder: 
# setwd("pathTo/gitsbe-model-analysis")
source("Rscripts/input_functions.R")
source("Rscripts/output_functions.R")
source("Rscripts/analysis_functions.R")
source("Rscripts/plot_functions.R")
```

## Input

We will define the name of the cell line which must match the name of the 
directory that has the input files inside the `cell-lines-2500` directory. Our
analysis in this notebook will be done on the data for the `A498` cell line:
```{r Input: cell line}
cell.line = "A498"
data.dir = paste0(getwd(), "/cell-lines-2500/", cell.line, "/")
```
**Three inputs** are used in this analysis:

- The **model_predictions** file which has for each model the prediction for 
each drug combination tested (*0* = no synergy predicted, *1* = synergy 
predicted, *NA* = couldn't find stable states in either the drug combination 
inhibited model or in any of the two single-drug inhibited models)
- The **observed_synergies** file which lists the drug combinations that were 
observed as synergistic for the particular cell line.
- The **models** directory, which is the same as the models directory produced 
by `Gitsbe` and has one `.gitsbe` file per model that includes this info:
    - The *stable state* of the boolean model. Note that a model can have 1 
stable state or none in our simulations - but the models used in this analysis 
have been selected through a genetic evolution algorithm in `Gitsbe` and so in 
the end, only those with 1 stable state have higher fitness values and remain in 
the later generations. Higher fitness here means a better match of a model's 
stable state to the cell line derived steady state (a perfect match would result 
in a fitness of 1)
    - The *boolean equations* of the model

```{r Input: files}
model.predictions.file = paste0(data.dir, "model_predictions")
observed.synergies.file = paste0(data.dir, "observed_synergies")
models.dir = paste0(data.dir, "models")
```
Now, we parse the data into proper R objects. First the synergy predictions 
per model:
```{r Input: model predictions}
model.predictions = get.model.predictions(model.predictions.file)
head(model.predictions)
```
So, we can see that our dataset has the models as rows and each column is a 
different drug combination that was tested in our simulations.
```{r Model&Drug Stats}
drug.combinations.tested = colnames(model.predictions)
models                   = rownames(model.predictions)
nodes                    = get.node.names(models.dir)

number.of.drug.comb.tested = length(drug.combinations.tested)
number.of.models           = length(models)
number.of.nodes            = length(nodes)

print.model.and.drug.stats(number.of.drug.comb.tested, number.of.models, 
                           number.of.nodes)
```
Next, we get the full stable state and the equations per model:
```{r Input: models stable states}
models.stable.state = get.stable.state.from.models.dir(models.dir)
models.stable.state = models.stable.state[models,]
head(as.data.frame(models.stable.state))
```
The rows of the above dataset represent the models while the columns are the
names of the nodes (proteins, genes, etc.) of the cancer cell network under
study. So, each model has one stable state which means that in every model, the
nodes in the network have reached a state of either 0 (inhibition) or 1
(activation).
```{r Input: models equations}
models.equations = get.equations.from.models.dir(
  models.dir, remove.equations.without.link.operator = TRUE)
models.equations = models.equations[models,]
head(as.data.frame(models.equations))
```
For the equations, if we look at a specific row (a model so to speak), the 
columns (node names) correspond to the *targets of regulation* (and the network 
has been built so that *every node is a target* - i.e. it has other nodes 
activating or inhibiting it). The general form of a boolean equation is: 
**Target \*= (Activator OR Activator OR...) AND NOT (Inhibitor OR Inhibitor 
OR...)**.

The difference between the models' boolean equations is the *link operator* 
(OR NOT/AND NOT) which has been mutated (changed) through the evolutionary 
process of the genetic algorithm in `Gitsbe`. For example, if a model has for 
the column `ERK_f` a value of 1, the correspoding equation is: 
ERK_f \*= (MEK_f) **OR NOT** ((DUSP6) OR PPP1CA). A value of 0 would correspond 
to the same equation but having **AND NOT**. Note that the equations that do not
have link operators (meaning that they are *the same for every model*) are 
discarded (so less columns in this dataset) since we want to see the 
change/difference between the models.

Lastly, the synergies observed for this particular cell line are:
```{r Input: observed synergies}
observed.synergies = get.observed.synergies(
  observed.synergies.file, drug.combinations.tested)
number.of.observed.synergies = length(observed.synergies)

print(paste("Number of synergies observed:", number.of.observed.synergies))
observed.synergies
```

## Analysis

### Synergy-based performance analysis

It will be interesting to know the percentage of the above observed synergies 
that were actually predicted by at least one of the models (there might be 
combinations that no model out of the 7500 could predict):
```{r Synergy stats 1}
# Subset the model.data to the observed synergies
observed.model.predictions = 
  model.predictions[,sapply(drug.combinations.tested, function(drug.comb) {
    is.correct.synergy(drug.comb, observed.synergies)
  })]

# Subset the model.data to the unobserved synergies
unobserved.model.predictions = 
  model.predictions[,sapply(drug.combinations.tested, function(drug.comb) {
    !is.correct.synergy(drug.comb, observed.synergies)
  })]

stopifnot(dim(observed.model.predictions)[2] + 
          dim(unobserved.model.predictions)[2] == number.of.drug.comb.tested)

number.of.models.per.observed.synergy = 
  apply(observed.model.predictions, 2, sum, na.rm = T)
predicted.synergies = names(number.of.models.per.observed.synergy)[
                            number.of.models.per.observed.synergy > 0]

number.of.models.per.observed.synergy
predicted.synergies
predicted.synergies.percentage = 100 * length(predicted.synergies) /
                    number.of.observed.synergies
print(paste0("Percentage of total predicted synergies (by at least one model): ", 
             specify.decimal(predicted.synergies.percentage, 2), "%"))
```
So, for the `A498` cell line, there were indeed synergies that no model could 
predict (e.g. `AK-BI`, `AK-PI`). Next, we would like to know the maximum number 
of observed synergies predicted by one model alone - can one model by itself 
predict all the 5 synergies that all the models together can predict or do we 
need many models to capture this diverse synergy landscape? To do that, we go 
even further and **count the number of models that predict a specific set of 
synergies** for every possible combination subset of the predicted synergy set 
found above:
```{r Synergy stats 2}
# Find the number of predictive models for every synergy subset
predicted.synergies.powerset = powerSet(predicted.synergies)
predicted.synergies.powerset = predicted.synergies.powerset[
  order(sapply(predicted.synergies.powerset, length))
]
names(predicted.synergies.powerset) = 
  sapply(predicted.synergies.powerset, function(drug.comb.set) {
    paste(drug.comb.set, collapse = ",")
  }) 
synergy.subset.stats = 
  sapply(predicted.synergies.powerset, function(drug.comb.set) {
    count.models.that.predict.synergy.set(drug.comb.set, observed.model.predictions) 
})

# Bar plot of the number of models for every possible observed synergy combination set
# Tweak the threshold.for.subset.removal and bottom.margin as desired
make.barplot.on.synergy.subset.stats(synergy.subset.stats,
                                     threshold.for.subset.removal = 1, 
                                     bottom.margin = 9, cell.line)
```

As can be seen from the figure above (where we excluded sets of synergies that 
were predicted by no model by setting the `threshold.for.subset.removal` value 
to 1), **almost half of the models predict no synergies**, while the `PI-D1` 
synergy is predicted by almost all of the rest of the models. Next we calculate 
the maximum number of correctly predicted synergies ($TP$ - True Positives) per 
model:
```{r TP Stats}
# Count the predictions of the observed synergies per model (TP)
models.synergies.tp = apply(observed.model.predictions, 1, sum, na.rm = T)
models.synergies.tp.stats = table(models.synergies.tp)

# Bar plot of number of models vs correctly predicted synergies
make.barplot.on.models.stats(models.synergies.tp.stats, cell.line, 
                            title = "True Positive Synergy Predictions",
                            xlab = "Number of maximum correctly predicted synergies",
                            ylab = "Number of models")
```

So, there were indeed **only 2 models that predicted 3 synergies** - the set 
`BI-PD,PD-PI,PI-D1` - which is the **maximum number of predicted synergies by an 
individual model**, whereas **no model could predict all 5 of the total predicted 
synergies**. The power of the ensemble model approach lies in the fact that (as 
we saw from the above figures) even though we may not have *individual super models* 
that can predict many observed drug combinations, there are many that predict 
*at least one* and which will be used by the drug response analysis module 
(`Drabme`) to better infer the synergistic drug combinations. It goes without 
saying though, that the existance of models that could predict more than a 
handful of synergies would be beneficial for any approach that performs drug 
response analysis on a multitude of models.

### Biomarker analysis

Now, we want to investigate and find possible important nodes - **biomarkers** -
whose activity state either distinguishes good performance models from less 
performant ones (in terms of a performance metric - e.g. maximum true positives 
predicted) or makes some models predict a specific synergy (or set of synergies) 
compared to others that can't (but could predict different ones). So, we devised 
*two strategies* to split the models in our disposal to *good* and *bad* ones 
(but not necessarily all of them), the demarcation line being either a performance 
metric (number of TP or Matthews Correlation Coefficient score) or 
the prediction or not of a specific synergy (or set of synergies). Then, for 
each group of models (labeled as either *good* or *bad*) we find the average 
activity state of every node in the network (value between 0 and 1) and then we 
compute the average difference for each node between the two groups:
$\forall i\in nodes,mean(state_i)_{good} - mean(state_i)_{bad}$. 
Our hypothesis is that if the absolute value of these average differences are 
larger than a user-defined threshold (e.g. 0.7) for a small number of nodes (the 
less the better) while for the rest of the nodes they remain close to zero, then 
the former nodes are considered **the most important** since they 
define the difference between the *average bad model* and the *average good one*
in that particular case study. We will also deploy a network visualizing method 
to observe these average differences.

#### True Positives-based analysis

Using our first strategy, **we will split the models based on the number of true 
positive predictions**. For example, the bad models will be the ones that 
predicted $0 TP$ synergies whereas the good models will be the ones that predicted 
$2 TP$ (we will denote the grouping as $(0,2)$). This particular classification 
strategy will be used for every possible combination of the number of TP as 
given by the `models.synergies.tp.stats` object and the density estimation of the
average node state differences in each case will be ploted in a common graph:
```{r Good vs Bad models based on the number of TPs}
tp.values = as.numeric(names(models.synergies.tp.stats))
tp.values.comb = t(combn(tp.values, 2))

diff.tp.results = apply(tp.values.comb, 1, function(comb) {
  return(get.avg.activity.diff.based.on.tp.predictions(
    models, models.synergies.tp, models.stable.state, 
    num.low = comb[1], num.high = comb[2]))
})

tp.comb.names = apply(tp.values.comb, 1, function(row) {
  return(paste0("(", paste(row, collapse = ","), ")")) 
})
colnames(diff.tp.results) = tp.comb.names
diff.tp.results = t(diff.tp.results)

tp.densities = apply(abs(diff.tp.results), 1, density)
make.multiple.density.plot(tp.densities, legend.title = "True Positives")
```

What we are actually looking for is density plots that are *largely skewed to 
the right* (so the average absolute differences of these nodes are close to zero)
while there are a few areas of non-zero densities which are as close to 1 as 
possible. So, from the above graph, the density plots that fit this description 
are the ones marked as $(1,3)$ and $(2,3)$. So, we can now visualize the 
average state differences in a network graph, where the color of each node will 
denote how much more inhibited or activated is the average good model vs the 
average bad one. The color of the edge will denote activation (green) or 
inhibition (red). We first build the network from the node topology (edge list):
```{r Input: topology}
parent.dir = get.parent.dir(data.dir)
topology.file = paste0(parent.dir, "/topology")

net = contruct.network(topology.file, models.dir)

# a static layout for plotting the same network always (igraph)
nice.layout = layout_nicely(net)
```
In the next colored graphs we can identify the important nodes whose activity
state can influence the true positive prediction performance (from 0 true 
positive synergies to a total of 3):
```{r Graphs: Good vs Bad models based on the number of TPs}
# useful tutorial about network visualization in R: (Ognyanova, 2008)

diff.tp.0.3 = diff.tp.results[3,]
plot.network(net, diff.tp.0.3, layout = nice.layout, 
             title = "Bad models (0 TP) vs Good models (3 TP)")

diff.tp.1.3 = diff.tp.results[5,]
plot.network(net, diff.tp.1.3, layout = nice.layout, 
             title = "Bad models (1 TP) vs Good models (3 TP)")

diff.tp.2.3 = diff.tp.results[6,]
plot.network(net, diff.tp.2.3, layout = nice.layout, 
             title = "Bad models (2 TP) vs Good models (3 TP)")
```

We will now **list the important nodes** that affect the models' prediction 
performance **with regards to the number of true positive synergies** that they 
predict. First, the nodes that have to be in a more *activated* state:
```{r}
threshold = 0.7
biomarkers.tp.activated = diff.tp.1.3[diff.tp.1.3 > threshold]
pretty.print.vector.names(biomarkers.tp.activated)
```
Secondly, the nodes that have to be in a more *inhibited* state:
```{r}
biomarkers.tp.inhibited = diff.tp.1.3[diff.tp.1.3 < -threshold]
pretty.print.vector.names(biomarkers.tp.inhibited)
```

#### MCC classification-based analysis

The previous method to split the models based on the number of true positive 
predictions is a good metric for absolute performance but a very restricted one 
since it ignores the other values of the confusion matrix for each model (true 
negatives, false positives, false negatives). Also, since our dataset is 
*imbalanced* in the sense that out of the total drug combinations tested only a 
few of them are observed as synergistic (and in a hypothetical larger drug 
screening evaluation it will be even less true positives) we will now devise a 
method to split the models into different performance categories based on the 
value of the **Matthews Correlation Coefficient (MCC) score** which takes into 
account the balance ratios of all the four confusion matrix values:
```{r MCC Stats}
# Count the false negatives (FN)
models.synergies.fn = apply(observed.model.predictions, 1, function(x) {
  sum( x == 0 | is.na(x) )
})

# P = TP + FN (Positives)
positives = ncol(observed.model.predictions)
models.synergies.p = models.synergies.tp + models.synergies.fn

# Count the predictions of the non-observed synergies per model (FP)
models.synergies.fp = apply(unobserved.model.predictions, 1, sum, na.rm = T)

# Count the True Negatives (TN)
models.synergies.tn = apply(unobserved.model.predictions, 1, function(x) {
  sum( x == 0 | is.na(x))
})

# N = FP + TN (Negatives)
negatives = ncol(unobserved.model.predictions)
models.synergies.n = models.synergies.fp + models.synergies.tn

# checks
stopifnot(models.synergies.p == positives)
stopifnot(models.synergies.n == negatives)
stopifnot(positives + negatives == number.of.drug.comb.tested)

# Calculate Matthews Correlation Coefficient (MCC)
models.mcc = calculate.mcc(models.synergies.tp, models.synergies.tn, 
                           models.synergies.fp, models.synergies.fn,
                           positives, negatives)
models.mcc.stats = table(models.mcc, useNA = "ifany")

make.barplot.on.models.stats(models.mcc.stats, cell.line, title = "MCC scores", 
                             xlab = "MCC value", ylab = "Number of models", 
                             cont.values = TRUE)
```

So, overall from the above figure we can see that there are no really bad models
(MCC values close to -1), while most of them perform are a little better than 
random prediction (MCC values > 0.1). Also, we note that there are models that 
produced `NaN` for the MCC score. Given the MCC formula:
$MCC = (TP\cdot TN - FP\cdot FN)/\sqrt{(TP+FP) * (TP+FN) * (TN+FP) * (TN+FN)}$,
we can see that this happens because of zero devision. Two of the four values in
the denominator represent the number of positive $(TP+FN)$ and negative $(TN+FP)$ 
observations which are non-zero for every model, since they correspond to 
observed and non-obsered synergies in each case (the same for all models of 
course). The case where both $TN$ and $FN$ are zero is rare (if non-existent) 
because of the imbalanced dataset (large proportion of negatives) and the reason 
that logical models which report no negatives means that they should find 
fixpoint attractors for every possible drug combination perturbation which also 
is unlikely. We can actually see that the `NaN` are produced by models that have
**both TP and FP equal to zero**:
```{r NaN MCC score}
sum(models.synergies.tp + models.synergies.fp == 0)
```
Since these models could intentify no synergies (either correctly or 
wrongly), we decided to put them as the lowest performant category in our 
MCC-based analysis. To classify the models based on the MCC score (which takes 
values in the $[-1, 1]$ interval), we devised a method that **splits the previously 
found MCC values range to intervals of a specific size** (each corresponding to a 
different classification category), so that each model's MCC score falls within 
a specific interval:
```{r MCC classification}
mcc.values = as.numeric(names(models.mcc.stats))

# split into classes with a 0.2 value range each
mcc.intervals = get.mcc.intervals(mcc.values, interval.size = 0.2)

# add NaN category (if applicable)
if (sum(is.na(mcc.values)) > 0) {
  mcc.intervals = rbind(c(NaN, NaN), mcc.intervals)
}

mcc.classes = get.mcc.classes(mcc.intervals)
print.mcc.classification.info(mcc.classes)
```
Following our first strategy, **we will split the models based on the MCC 
performance metric score**. For example, the bad models will be the ones that 
had a `NaN` MCC score $(TP+FP = 0)$ whereas the good models will be the ones that 
had an MCC score between $[-0.2, 0)$. This particular classification strategy will 
be used for every possible combination of the MCC classes as defined by the 
`mcc.classes` object and the density estimation of the average node state 
differences in each case will be ploted in a common graph:
```{r Good vs Bad models based on the MCC classification}
mcc.class.id.comb = t(combn(1:length(mcc.classes), 2))

diff.mcc.results = apply(mcc.class.id.comb, 1, function(comb) {
  return(get.avg.activity.diff.based.on.mcc.classification(
    models, models.mcc, mcc.intervals, models.stable.state, 
    class.id.low = comb[1], class.id.high = comb[2]))
})

mcc.classes.comb.names = apply(mcc.class.id.comb, 1, function(comb) {
  return(paste0("(", mcc.classes[comb[1]], ", ", mcc.classes[comb[2]], ")"))
})

colnames(diff.mcc.results) = mcc.classes.comb.names
diff.mcc.results = t(diff.mcc.results)

densities = apply(abs(diff.mcc.results), 1, density)
make.multiple.density.plot(densities, legend.title = "MCC classes")
```

From the above graph we notice that the `NaN` MCC class has almost the same 
density distribution of the absolute average state differences when compared to
all the other MCC classes, meaning that this truly represents a (bad) class
of models which are fundamentally different from all other models that have at 
least one TP or FP prediction. The density plots of interest here are the ones 
that compare the **2nd vs 4th class** $([-0.2, 0), [0.2, 0.4])$ and the **3rd vs 
4th class** $([0, 0.2), [0.2, 0.4])$. Next, we visualize the average state 
differences with our network coloring method, in order to identify the important 
nodes whose activity state can influence the prediction performance based on the 
MCC classification:
```{r Graphs: Good vs Bad models based on the MCC classification}
# ([-0.2, 0), [0.2, 0.4])
diff.mcc.2.4 = diff.mcc.results[5,]
plot.network(net, diff.mcc.2.4, layout = nice.layout, 
             title = "Bad models, MCC: [-0.2, 0) vs Good models, MCC: [0.2, 0.4]")

# ([0, 0.2), [0.2, 0.4])
diff.mcc.3.4 = diff.mcc.results[6,]
plot.network(net, diff.mcc.3.4, layout = nice.layout, 
             title = "Bad models, MCC: [0, 0.2) vs Good models, MCC: [0.2, 0.4]")
```

We will now **list the important nodes** that affect the models' prediction 
performance **with regards to their MCC classification**. First, the nodes that 
have to be in a more *activated* state:
```{r}
biomarkers.mcc.activated = diff.mcc.2.4[diff.mcc.2.4 > threshold]
pretty.print.vector.names(biomarkers.mcc.activated)
```
Secondly, the nodes that have to be in a more *inhibited* state:
```{r}
biomarkers.mcc.inhibited = diff.mcc.2.4[diff.mcc.2.4 < -threshold]
pretty.print.vector.names(biomarkers.mcc.inhibited)
```

Comparing the two methods we used for the classification of the models' prediction
performance (TP and MCC), we observe that there exist **common biomarkers** in 
both activated and inhibited states. Also we note that there weren't any common 
nodes reported as significantly activated in one method which were significantly 
inhibited in the other, showing that the two methods' results correlate well:
```{r}
print.common.nodes(biomarkers.mcc.activated, biomarkers.tp.inhibited)
print.common.nodes(biomarkers.mcc.inhibited, biomarkers.tp.activated)
```

To sum up, we list the common biomarkers that need to be in a more *activated* 
state:
```{r}
print.common.nodes(biomarkers.mcc.activated, biomarkers.tp.activated)
```
We also list the common nodes that have to be in a more *inhibited* state:
```{r}
print.common.nodes(biomarkers.mcc.inhibited, biomarkers.tp.inhibited)
```


- threejs
- networkD3
?igraph_to_networkD3
```{r Comparison of network visualization libraries}
library("visNetwork")

# ([0, 0.2), [0.2, 0.4])
diff.mcc.3.4 = diff.mcc.results[6,]
plot.network(net, diff.mcc.3.4, layout = nice.layout,
             title = "Bad models, MCC: [0, 0.2) vs Good models, MCC: [0.2, 0.4]")
plot.network.vis(net, diff.mcc.3.4, layout = nice.layout,
             title = "Bad models, MCC: [0, 0.2) vs Good models, MCC: [0.2, 0.4]")
```




#### Synergy-prediction based analysis

We will now use the second strategy to split the models, namely based on whether 
they predict a specific synergy or not. 
Two synergies of interest in A498 cell line `AK-PD`, `PD-PI`


Fix the two functions:
- One synergy predicted vs rest of the models who can't predict it
- Common model stuff, multiple set comparison



```{r eval=FALSE, include=FALSE}
# Good vs Bad models based on the number of the models
# that predicted a specific synergy
predicted.synergies = synergy_observations_data
for (drugComb in synergy_observations_data) {
  diff_specific = get.diff.specific.synergy(drugComb, observed.modelData, models_stable_state)
  if (sum(is.na(diff_specific)) == length((diff_specific))) {
    print(paste("No models predicted the ", drugComb, " synergy", sep = ""))
    predicted.synergies = predicted.synergies[! predicted.synergies %in% drugComb]
  } else {
    #output.diff.to.file(cell_line, drugComb, diff_specific)
  }
}
non.predicted.synergies = synergy_observations_data[!synergy_observations_data %in% predicted.synergies]

# Good vs Bad models when comparing models that predicted diferrent synergy sets
# Usually good models predicted: (A-B, C-D, K-L) and bad (A-B, C-D) - just to see
# what the good models have that allows them to predict the K-L synergy also
diff.on.synergy.sets = 
  get.diff.from.models.predicting.diff.synergy.sets(all.synergy.subsets["AK-D1,BI-D1,PK-ST"], all.synergy.subsets["AK-BI,AK-D1,BI-D1,PK-ST"], observed.modelData, models_stable_state)
```









# References

1. Csardi G, Nepusz T (2006) The igraph software package for complex network research, InterJournal, Complex Systems 1695. [http://igraph.org](http://igraph.org)
2. Ognyanova, K. (2018) Network visualization with R. Retrieved from [www.kateto.net/network-visualization](http://www.kateto.net/network-visualization)
