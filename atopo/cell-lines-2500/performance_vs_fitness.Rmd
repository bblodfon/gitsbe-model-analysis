---
title: "Performance vs Fitness Model Analysis"
author: "[John Zobolas](https://github.com/bblodfon)"
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: united
    toc: true
    toc_float: true
    number_sections: false
    code_folding: hide
    code_download: true
---

## Intro {-}

The purpose of this analysis is to find a correlation between the boolean models
**fitness to a steady state activity profile** and their **performance** in terms of the
number of *True Positive* (TP) synergies predicted and/or the overall *MCC score*
(Matthews Correlation Coefficient score). We want to show that **a closer fitness 
to the steady state suggests more predictive models**, corroborating thus our proof of concept of using an ensemble-based approach where models are trained towards a 
specific steady state signaling pattern for drug combination predictions.

The boolean model datasets we will use are in total $9$: one for each cell line 
of interest (8 cell lines) where the models were **fitted to a specific steady state** in each 
case and one for the so-called **random models** which were generated *randomly* in 
the sense that were fitted only to a proliferation state (simulations were done using 
the DrugLogics software modules `Gitsbe` and `Drabme`).

Each boolean model dataset constitues of:

- The **model predictions** file which has for each model the prediction for 
each drug combination tested (*0* = no synergy predicted, *1* = synergy 
predicted, *NA* = couldn't find stable states in either the drug combination 
inhibited model or in any of the two single-drug inhibited models)
- The **models stable state** (one per model). A **fitness score**
for each model can easily be calculated then by matching the model's stable 
state (which is something inherent in the boolean's model structure, a unique 
fixpoint attractor) with the steady state of interest, node per node.
A **higher fitness score** would mean a better match of a model's 
stable state to the cell line derived steady state (a perfect match would result 
in a fitness of 1).
- The **observed synergies** file which lists the drug combinations that were 
observed as synergistic for each cell line.
- The **steady state** file which lists the network nodes (protein, gene, complexes
names, etc) and their activity value (0 or 1, representing an inhibited or active
node respectvely). 
This input is provided per cell line and not for the random models since they are just trained to a profileration state.

```{r Load libraries, message = FALSE, echo = FALSE}
library(DT)
library(usefun)
library(emba)
```

## Input {-}

First we load the cell-specific input data:
```{r Cell-specific Input}
# Cell Lines
cell.lines = c("A498", "AGS", "DU145", "colo205", "SW620", "SF295", "UACC62", "MDA-MB-468")

cell.line.dirs = sapply(cell.lines, function(cell.line) {
  paste0(getwd(), "/", cell.line)
})

# Model predictions
model.predictions.files = sapply(cell.line.dirs, function(cell.line.dir) {
  paste0(cell.line.dir, "/model_predictions")
})

model.predictions.per.cell.line = lapply(model.predictions.files, 
  function(file) {
    get_model_predictions(file)
  }
)

# Observed synergies
observed.synergies.files = sapply(cell.line.dirs, function(cell.line.dir) {
  paste0(cell.line.dir, "/observed_synergies")
})

observed.synergies.per.cell.line = lapply(observed.synergies.files, 
  function(file) {
    get_observed_synergies(file)
  }
)

# Models Stable State (1 per model)
models.stable.state.files = sapply(cell.line.dirs, function(cell.line.dir) {
  paste0(cell.line.dir, "/models_stable_state")
})

models.stable.state.per.cell.line = lapply(models.stable.state.files,
  function(file) {
    as.matrix(read.table(file, check.names = FALSE))
  }
)

# the node names used in our analysis
node.names = colnames(models.stable.state.per.cell.line[[1]])

# Steady States
steady.state.files = sapply(cell.line.dirs, function(cell.line.dir) {
  paste0(cell.line.dir, "/steady_state")
})

steady.state.per.cell.line = lapply(steady.state.files,
  function(file) {
    ss.df = read.table(file, sep = "\t", stringsAsFactors = FALSE)
    steady.state = ss.df[,2]
    names(steady.state) = ss.df[,1]
    
    # change value to NA for nodes for which there was no activity found (dash)
    steady.state[steady.state == "-"] = NA
    
    # keep only the nodes that are included in the analysis
    steady.state = prune_and_reorder_vector(steady.state, node.names)
    
    # return an integer vector since the activity values are binarized (0,1)
    return(sapply(steady.state, as.integer))
  }
)
```

For the random models, we just need to get the predictions and stable states
for each model:
```{r Random model Input}
random.dir = paste0(getwd(), "/random")
random.model.predictions = get_model_predictions(paste0(random.dir, "/model_predictions"))

random.models.stable.state = as.matrix(
  read.table(file = paste0(random.dir, "/models_stable_state"), check.names = FALSE)
)
```

## Model Analysis {-}

In order to find the number of true positive predicted synergies, MCC scores and fitness scores for each of the models in each of the 9 datasets, we use the generic analysis functions from the [emba](https://github.com/bblodfon/emba) R package.

### Cell-specific {-}

The full analysis results for each cell line:
```{r Cell-specific Model Analysis (MCC and TP-based), cache=TRUE}
# Performance Biomarkers for cell specific models (MCC)
cell.specific.mcc.analysis.res = list()
for (cell.line in cell.lines) {
  cell.specific.mcc.analysis.res[[cell.line]] =
    biomarker_mcc_analysis(model.predictions = model.predictions.per.cell.line[[cell.line]],
                           models.stable.state = models.stable.state.per.cell.line[[cell.line]],
                           models.link.operator = NULL,
                           observed.synergies = observed.synergies.per.cell.line[[cell.line]],
                           threshold = 0.7, num.of.mcc.classes = 5,
                           include.NaN.mcc.class = FALSE)
  }

# Performance Biomarkers for cell specific models (TP)
cell.specific.tp.analysis.res = list()
for (cell.line in cell.lines) {
  cell.specific.tp.analysis.res[[cell.line]] =
    biomarker_tp_analysis(model.predictions = model.predictions.per.cell.line[[cell.line]],
                          models.stable.state = models.stable.state.per.cell.line[[cell.line]],
                          models.link.operator = NULL,
                          observed.synergies = observed.synergies.per.cell.line[[cell.line]],
                          threshold = 0.7)
  }
```

Finding the MCC, TP and fitness values for each model from the above result (note
that each model's stable state in a specific cell line is matched against the 
steady state from that cell line):
```{r Cell-specific Models TP + MCC + fitness per cell line}
models.mcc.per.cell.line = list()
for (cell.line in cell.lines) {
  models.mcc.per.cell.line[[cell.line]] = 
    cell.specific.mcc.analysis.res[[cell.line]][["models.mcc"]]
}

models.tp.per.cell.line = list()
for (cell.line in cell.lines) {
  models.tp.per.cell.line[[cell.line]] = 
    cell.specific.tp.analysis.res[[cell.line]][["models.synergies.tp"]]
}

models.fitness.per.cell.line = list()
for (cell.line in cell.lines) {
  models.fitness.per.cell.line[[cell.line]] = 
    apply(models.stable.state.per.cell.line[[cell.line]], 1, get_percentage_of_matches, 
          steady.state.per.cell.line[[cell.line]])
}
```

### Random Models {-}

The full analysis results for the random models (what changes here is only 
the **observed synergies** per cell line):
```{r Random Model Analysis (MCC and TP-based), cache=TRUE}
# Performance Biomarkers for cell proliferation models (MCC)
random.mcc.analysis.res = 
  lapply(observed.synergies.per.cell.line, function(observed.synergies) {
    biomarker_mcc_analysis(random.model.predictions, 
                           random.models.stable.state,
                           models.link.operator = NULL, 
                           observed.synergies, threshold = 0.6, 
                           num.of.mcc.classes = 5, include.NaN.mcc.class = FALSE
                           )})

# Performance Biomarkers for cell proliferation models (TP)
random.tp.analysis.res = 
  lapply(observed.synergies.per.cell.line, function(observed.synergies) {
    biomarker_tp_analysis(random.model.predictions, 
                          random.models.stable.state,
                          models.link.operator = NULL, 
                          observed.synergies, threshold = 0.6
                          )})
```

Finding the MCC, TP and fitness values for each model from the above result (note
that each model's stable state in a specific cell line is matched against the 
steady state from that cell line and that the random models' stable state data
does not change per cell line, i.e. same `random.models.stable.state` object):
```{r Random Models TP + MCC + fitness per cell line}
random.models.mcc.per.cell.line = list()
for (cell.line in cell.lines) {
  random.models.mcc.per.cell.line[[cell.line]] = 
    random.mcc.analysis.res[[cell.line]][["models.mcc"]]
}

random.models.tp.per.cell.line = list()
for (cell.line in cell.lines) {
  random.models.tp.per.cell.line[[cell.line]] = 
    random.tp.analysis.res[[cell.line]][["models.synergies.tp"]]
}

random.models.fitness.per.cell.line = list()
for (cell.line in cell.lines) {
  random.models.fitness.per.cell.line[[cell.line]] = 
    apply(random.models.stable.state, 1, get_percentage_of_matches, 
          steady.state.per.cell.line[[cell.line]])
}
```

## Choosing best dataset for analysis {-}

We now want to **find the best dataset & cell line** for our subsequent analysis - that is to show the performance vs fitness correlation. 
The argument here is that we want to choose a boolean dataset that has a **large enough fitness value range** along with a large **TP and/or MCC value range**, since with smaller value ranges it would be harder to distinguish the difference of the estimated distributions of the fitness scores belonging to different performance classes (i.e. models' fitnesses that belong to different classification groups with the metric being either the number of TPs or the MCC score).

The next two tables will help us determine exactly which cell line and dataset
to use:
```{r Cell-specific Models TP + MCC + fitness per cell line stats}
cell.specific.model.data = matrix(data = NA, nrow = length(cell.lines), ncol = 8)
rownames(cell.specific.model.data) = cell.lines
colnames(cell.specific.model.data) = c("fitness range", "Min fitness", 
  "Max fitness", "MCC range", "Min MCC", "Max MCC", "TP range", "Max TP")

for (cell.line in cell.lines) {
  models.fitness = models.fitness.per.cell.line[[cell.line]]
  range.fitness = range(models.fitness)
  
  models.tp = models.tp.per.cell.line[[cell.line]]
  range.tp = range(models.tp)
  
  models.mcc = models.mcc.per.cell.line[[cell.line]]
  # TP + FP = 0 for some of the models (see each respective model analysis)
  # so we need to remove NaNs
  range.mcc = range(models.mcc, na.rm = TRUE)
  
  cell.specific.model.data[cell.line, "fitness range"] = range.fitness[2] - range.fitness[1]
  cell.specific.model.data[cell.line, "Min fitness"] = range.fitness[1]
  cell.specific.model.data[cell.line, "Max fitness"] = range.fitness[2]
  
  cell.specific.model.data[cell.line, "MCC range"] = range.mcc[2] - range.mcc[1]
  cell.specific.model.data[cell.line, "Min MCC"] = range.mcc[1]
  cell.specific.model.data[cell.line, "Max MCC"] = range.mcc[2]
  
  cell.specific.model.data[cell.line, "TP range"] = range.tp[2] - range.tp[1]
  cell.specific.model.data[cell.line, "Max TP"] = range.tp[2]
}

# color columns
fit.breaks = quantile(cell.specific.model.data[,"fitness range"], probs = seq(.05, .95, .05), na.rm = TRUE)
fit.colors = round(seq(255, 40, length.out = length(fit.breaks) + 1), 0) %>%
  {paste0("rgb(255,", ., ",", ., ")")} # red
mcc.breaks = quantile(cell.specific.model.data[,"MCC range"], probs = seq(.05, .95, .05), na.rm = TRUE)
mcc.colors = round(seq(255, 40, length.out = length(mcc.breaks) + 1), 0) %>%
  {paste0("rgb(", ., ",255,", ., ")")} # green

caption.title = "Table 1: Fitness, MCC scores and TP (True positives) for the Cell-specific model predictions across 8 Cell Lines"
datatable(data = cell.specific.model.data, 
          options = list(dom = "t"), # just show the table
          caption = htmltools::tags$caption(caption.title, style="color:#dd4814; font-size: 18px")) %>% 
  formatRound(c(1,2,3,4,5,6), digits = 3) %>%
  formatStyle(columns = c("fitness range"), backgroundColor = styleInterval(fit.breaks, fit.colors)) %>%
  formatStyle(columns = c("MCC range"), backgroundColor = styleInterval(mcc.breaks, mcc.colors))
```

```{r Random Models TP + MCC + fitness per cell line stats}
random.model.data = matrix(data = NA, nrow = length(cell.lines), ncol = 8)
rownames(random.model.data) = cell.lines
colnames(random.model.data) = c("fitness range", "Min fitness", "Max fitness", 
  "MCC range", "Min MCC", "Max MCC", "TP range", "Max TP")

for (cell.line in cell.lines) {
  models.fitness = random.models.fitness.per.cell.line[[cell.line]]
  range.fitness = range(models.fitness)
  
  models.tp = random.models.tp.per.cell.line[[cell.line]]
  range.tp = range(models.tp)
  
  models.mcc = random.models.mcc.per.cell.line[[cell.line]]
  range.mcc = range(models.mcc)
  
  random.model.data[cell.line, "fitness range"] = range.fitness[2] - range.fitness[1]
  random.model.data[cell.line, "Min fitness"] = range.fitness[1]
  random.model.data[cell.line, "Max fitness"] = range.fitness[2]
  
  random.model.data[cell.line, "MCC range"] = range.mcc[2] - range.mcc[1]
  random.model.data[cell.line, "Min MCC"] = range.mcc[1]
  random.model.data[cell.line, "Max MCC"] = range.mcc[2]
  
  random.model.data[cell.line, "TP range"] = range.tp[2] - range.tp[1]
  random.model.data[cell.line, "Max TP"] = range.tp[2]
}

# color columns
fit.breaks = quantile(random.model.data[,"fitness range"], probs = seq(.05, .95, .05), na.rm = TRUE)
fit.colors = round(seq(255, 40, length.out = length(fit.breaks) + 1), 0) %>%
  {paste0("rgb(255,", ., ",", ., ")")} # red
mcc.breaks = quantile(random.model.data[,"MCC range"], probs = seq(.05, .95, .05), na.rm = TRUE)
mcc.colors = round(seq(255, 40, length.out = length(mcc.breaks) + 1), 0) %>%
  {paste0("rgb(", ., ",255,", ., ")")} # green

caption.title = "Table 2: Fitness, MCC scores and TP (True positives) for the random model predictions across 8 Cell Lines"
datatable(data = random.model.data, 
          options = list(dom = "t"), # just show the table
          caption = htmltools::tags$caption(caption.title, style="color:#dd4814; font-size: 18px")) %>% 
  formatRound(c(1,2,3,4,5,6), digits = 3) %>%
  formatStyle(columns = c("fitness range"), backgroundColor = styleInterval(fit.breaks, fit.colors)) %>%
  formatStyle(columns = c("MCC range"), backgroundColor = styleInterval(mcc.breaks, mcc.colors))
```

In general, we observe that the **random models offer a larger range of 
fitness values** when their stable states are matched against the steady state 
of each particular cell line. 
This is something we expected since these models weren't fitted to a 
specific cell-line steady state (meaning that they weren't chosen from the 
`Gitsbe` module as the 3 best from each simulation that match that steady state 
as best as possible) but rather to a more generic state of proliferation. 
Thus, they represent a set of models with larger variation in terms of 
structure (boolean model equations) compared to the cell-specific generated ones.

Also, note that the **maximum fitness value** is almost always larger in the 
cell-specific models vs the random ones for each respective cell line.

All in all, we will use the **random models data**, contrasted to the steady 
state of the `A498` cell line (see Table 2). This dataset has the largest 
fitness, MCC and TP value range combined in both tables above.

## Statistical Analysis {-}

- TP-classfication

- MCC-classfication

How many classes to split the models in MCC (2 - many?) and how many fitness 
values to choose from each class? (solved in previous code with the samples)

```{r Stats, eval=FALSE, include=FALSE}
# Subset the model.data to the observed synergies
observed.model.predictions = 
  model.predictions[,sapply(drug.combinations.tested, function(drug.comb) {
    is.correct.synergy(drug.comb, observed.synergies)
  })]

# Subset the model.data to the unobserved synergies
unobserved.model.predictions = 
  model.predictions[,sapply(drug.combinations.tested, function(drug.comb) {
    !is.correct.synergy(drug.comb, observed.synergies)
  })]

stopifnot(dim(observed.model.predictions)[2] + 
          dim(unobserved.model.predictions)[2] == number.of.drug.comb.tested)

# Count the predictions of the observed synergies per model (TP)
models.synergies.tp = apply(observed.model.predictions, 1, sum, na.rm = T)
models.synergies.tp.stats = table(models.synergies.tp)
tp.values = as.numeric(names(models.synergies.tp.stats))

# Bar plot of number of models vs correctly predicted synergies
make.barplot.on.models.stats(models.synergies.tp.stats, cell.line, 
                            title = "True Positive Synergy Predictions",
                            xlab = "Number of maximum correctly predicted synergies",
                            ylab = "Number of models")
```


```{r TP-classification of fitness values, eval=FALSE, include=FALSE}
fitness.tp = list(length(tp.values))

i = 0
for (value in tp.values) {
  i = i + 1
  fitness.tp[[i]] = models.fitness[models.synergies.tp == value]
}
names(fitness.tp) = tp.values

# Hypothesis testing
kruskal.test(fitness.tp)

# ploting data (values and density estimations)
cols = brewer.pal(length(tp.values), "Set1")
boxplot(fitness.tp, main = "Fitness vs Performance (TP)", 
        xlab = "Number of TP predictions",
        ylab = "Fitness score",
        col = cols)

densities = lapply(fitness.tp, density)
make.multiple.density.plot(densities, legend.title = "#TP",
        title = "Density Estimation", x.axis.label = "Fitness score")
```


```{r Correlation Fitness vs number of TP predictions, fig.height=15, fig.width=7, eval=FALSE, include=FALSE}

SOS: No correlation between fitness and number of TP predictions:


sample.size = 100
number.of.samples = 5

par(mfrow = c(number.of.samples, 2))
for (num in 1:number.of.samples) {
  # sampling data
  fitness.tp.sampled = lapply(fitness.tp, function(fit.vector) {
    return(sample(fit.vector, size = sample.size, replace = FALSE))
  })
  
  # Null hypothesis testing: the location parameters of the distribution
  # of `fitness.tp.sampled` are the same in each group (sample)
  kw.test.res = kruskal.test(fitness.tp.sampled)
  
  # ploting data (values and density estimations)
  cols = brewer.pal(length(tp.values), "Set1")
  boxplot(fitness.tp.sampled, main = "Fitness vs Performance (TP)", 
          xlab = "Number of TP predictions",
          ylab = "Fitness score",
          col = cols)
  
  densities = lapply(fitness.tp.sampled, density)
  make.multiple.density.plot(densities, legend.title = "#TP",
          title = "Density Estimation", x.axis.label = "Fitness score")
  legend("topleft", legend = 
          paste("p-value =", specify.decimal(kw.test.res$p.value, 3)))
}
```


```{r Calculate MCC, eval=FALSE, include=FALSE}
# Count the false negatives (FN)
models.synergies.fn = apply(observed.model.predictions, 1, function(x) {
  sum( x == 0 | is.na(x) )
})

# P = TP + FN (Positives)
positives = ncol(observed.model.predictions)
models.synergies.p = models.synergies.tp + models.synergies.fn

# Count the predictions of the non-observed synergies per model (FP)
models.synergies.fp = apply(unobserved.model.predictions, 1, sum, na.rm = T)

# Count the True Negatives (TN)
models.synergies.tn = apply(unobserved.model.predictions, 1, function(x) {
  sum( x == 0 | is.na(x))
})

# N = FP + TN (Negatives)
negatives = ncol(unobserved.model.predictions)
models.synergies.n = models.synergies.fp + models.synergies.tn

# checks
stopifnot(models.synergies.p == positives)
stopifnot(models.synergies.n == negatives)
stopifnot(positives + negatives == number.of.drug.comb.tested)

# Calculate Matthews Correlation Coefficient (MCC)
models.mcc = calculate.mcc(models.synergies.tp, models.synergies.tn, 
                           models.synergies.fp, models.synergies.fn,
                           positives, negatives)
models.mcc.stats = table(models.mcc, useNA = "ifany")
print("MCC value range:")
range(models.mcc, na.rm = TRUE)

# NaN MCC score
sum(models.synergies.tp + models.synergies.fp == 0)

# MCC stats
# show only mcc values >= 100
make.barplot.on.models.stats(models.mcc.stats, cell.line, title = "MCC scores", 
                             xlab = "MCC value", ylab = "Number of models", 
                             cont.values = TRUE, threshold = 99)

# MCC classification
mcc.values = as.numeric(names(models.mcc.stats))

# split into classes with a 0.2 MCC value range each
mcc.intervals = get.mcc.intervals(mcc.values, interval.size = 0.2)

# add NaN category (if applicable)
if (sum(is.na(mcc.values)) > 0) {
  mcc.intervals = rbind(c(NaN, NaN), mcc.intervals)
}

mcc.classes = get.mcc.classes(mcc.intervals)
print.mcc.classification.info(mcc.classes)
```


```{r Calculate other measures of performance evaluation, eval=FALSE, include=FALSE}
# False Positive Rate (FPR), Accuracy
models.tpr = models.synergies.tp / models.synergies.p # Sensitivity/Recall
models.tnr = models.synergies.tn / models.synergies.n # Specificity
models.ppv = models.synergies.tp / (models.synergies.tp + models.synergies.fp) # precision
models.fpr = models.synergies.fp / models.synergies.n
models.accuracy = (models.synergies.tp + models.synergies.tn) / 
                  (models.synergies.p + models.synergies.n)
models.balanced.accuracy = (models.tpr + models.tnr)/2
```

```{r Graphs: Fitness vs Evaluation metrics, eval=FALSE, include=FALSE}
fitness.vs.avg.fp = 
  get.average.over.unique.values(models.fitness, models.synergies.fp)
fitness.vs.avg.tp = 
  get.average.over.unique.values(models.fitness, models.synergies.tp)
fitness.vs.avg.fn = 
  get.average.over.unique.values(models.fitness, models.synergies.fn)
fitness.vs.avg.tn = 
  get.average.over.unique.values(models.fitness, models.synergies.tn)

x = fitness.vs.avg.fp[,1]
avg = fitness.vs.avg.fp[,2]
sdev = fitness.vs.avg.fp[,3]
plot(fitness.vs.avg.fp, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "Number of FP predictions",
     main = "Average FP per unique fitness score")
#arrows(x, avg-sdev, x, avg+sdev, length=0.05, angle=90, code=3)


x = fitness.vs.avg.tp[,1]
avg = fitness.vs.avg.tp[,2]
sdev = fitness.vs.avg.tp[,3]
plot(fitness.vs.avg.tp, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "Number of TP predictions",
     main = "Average TP per unique fitness score")

x = fitness.vs.avg.tn[,1]
avg = fitness.vs.avg.tn[,2]
sdev = fitness.vs.avg.tn[,3]
plot(fitness.vs.avg.tn, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "Number of TN predictions",
     main = "Average TN per unique fitness score")

x = fitness.vs.avg.fn[,1]
avg = fitness.vs.avg.fn[,2]
sdev = fitness.vs.avg.fn[,3]
plot(fitness.vs.avg.fn, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "Number of FN predictions",
     main = "Average FN per unique fitness score")
```

```{r, eval=FALSE, include=FALSE}
fitness.vs.avg.tpr = 
  get.average.over.unique.values(models.fitness, models.tpr)
fitness.vs.avg.tnr = 
  get.average.over.unique.values(models.fitness, models.tnr)
fitness.vs.avg.ppv = 
  get.average.over.unique.values(models.fitness, models.ppv)
fitness.vs.avg.fpr = 
  get.average.over.unique.values(models.fitness, models.fpr)
fitness.vs.avg.accuracy = 
  get.average.over.unique.values(models.fitness, models.accuracy)

x = fitness.vs.avg.tpr[,1]
avg = fitness.vs.avg.tpr[,2]
sdev = fitness.vs.avg.tpr[,3]
plot(fitness.vs.avg.tpr, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "TPR",
     main = "Average TPR per unique fitness score")

x = fitness.vs.avg.tnr[,1]
avg = fitness.vs.avg.tnr[,2]
sdev = fitness.vs.avg.tnr[,3]
plot(fitness.vs.avg.tnr, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "TNR",
     main = "Average TNR per unique fitness score")

# Nan problem

# x = fitness.vs.avg.ppv[,1]
# avg = fitness.vs.avg.ppv[,2]
# sdev = fitness.vs.avg.ppv[,3]
# plot(fitness.vs.avg.ppv, ylim=range(c(avg-sdev, avg+sdev)), 
#      xlab = "Fitness Score", ylab = "PPV")

x = fitness.vs.avg.fpr[,1]
avg = fitness.vs.avg.fpr[,2]
sdev = fitness.vs.avg.fpr[,3]
plot(fitness.vs.avg.fpr, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "FPR",
     main = "Average FPR per unique fitness score")

x = fitness.vs.avg.accuracy[,1]
avg = fitness.vs.avg.accuracy[,2]
sdev = fitness.vs.avg.accuracy[,3]
plot(fitness.vs.avg.accuracy, ylim=range(c(avg-sdev, avg+sdev)), 
     xlab = "Fitness Score", ylab = "Accuracy",
     main = "Average Accuracy per unique fitness score")
```

## R session info {-}

```{r session info, comment=""}
xfun::session_info()
```
