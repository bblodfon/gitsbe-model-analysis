---
title: "Random model predictions (CASCADE 2.0) vs Number of Stable States"
author: "[John Zobolas](https://github.com/bblodfon)"
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
description: "An investigation"
url: 'https\://bblodfon.github.io/gitsbe-model-analysis/cascade/random-model-ss/index.html'
github-repo: "bblodfon/gitsbe-model-analysis"
link-citations: true
site: bookdown::bookdown_site
---

# Intro {-}

:::{.green-box}
Main question: is there a relation between *random* models **stable state number** and their **performance** as measured by AUC PR (Precision-Recall) or AUC ROC (Receiver Operatoring Characteristic)?
:::

This investigation can be considered as an extension of the random model ROC and PR curves I created for the AGS paper I (see [here](https://bblodfon.github.io/ags-paper-1/index.html)).
I took there from the $3000$ model sample, only the models that had 1 stable state to compute their ROC performance and maybe that wasn't a good sampling :)

# Input {-}

The *random link-operator mutated* models were generated from the **CASCADE 2.0** topology, using the [abmlog software](https://github.com/druglogics/abmlog), version `1.6.0`.
Their prediction performance was assessed by the `Drabme` software module, via the `druglogics-synergy` Java module, version `1.2.0`.

I run the following command to get the random models from the `abmlog` repo root (took ~165 min to **generate all $50000$ models**):
```{r, eval=FALSE}
java -cp target/abmlog-1.6.0-jar-with-dependencies.jar eu.druglogics.abmlog.RandomBooleanModelGenerator --file=test/cascade_2_0.sif --num=50000
```

---

I splitted the models to **3 groups**: those that have no 1 stable state, 2 stable states or none at all.
There was only 1 model in $50000$ (`cascade_2_0_418669279901714.gitsbe`) that had 3 stable states and so we will not take into account models with more than 2 stable states (very rare and almost impossible to be chosen via `Gitsbe` since it indirectly penalizes models with more than 1 attractors).
The distribution of models in each category were (use the [count_ss.sh](https://raw.githubusercontent.com/bblodfon/gitsbe-model-analysis/master/cascade/random-model-ss/data/count_ss.sh) script):

| #Stable states | #Models | Percentage |
|---|---|---|
| 0 | 29027 | 58% |
| 1 | 20672 | 41% |
| 2 | 300   | 1%  |
| 3 | 1     | less than 0.01%  |

From the `ags_cascade_2.0` directory of the `druglogics-synergy` module I ran the [run.sh](https://raw.githubusercontent.com/bblodfon/gitsbe-model-analysis/master/cascade/random-model-ss/data/run.sh) script.
This script bootstraps $50$ models ($20$ times in total) from each category and runs the Drabme with those.
So bootstraping $50$ models $\times \text{ }20$ times with 0 stable states, $50$ models $\times \text{ }20$ times with 1 stable state, etc.
We also try all pair-wise combinations: {($25$ models with 0 stable states) + ($25$ models with 1 stable state)} $\times\text{ }20$ times, etc.
Lastly, we merge all of the different stable state models together in a pool of $25\times3=75$ models (again bootstraping $20$ such samples).

:::{.orange-box}
The generated random models and the results of the `Drabme` simulations are stored in 
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3932382.svg)](https://doi.org/10.5281/zenodo.3932382)
:::

# Libraries {-}

```{r Load libraries, message = FALSE}
library(emba)
library(stringr)
library(xfun)
library(PRROC)
library(dplyr)
library(ggpubr)
```

# Analysis {-}

Note that `data_dir` in the next code-block is the directory where you uncompress the Zenodo dataset file (`tar -xzvf boot_random_ss.tar.gz`).
The `ss_class` denotes the model group.
E.g. `ss0` is the group with models with zero stable state, `ss12` is the group with half the models having 1 stable state and half of the others having 2 stable states, etc.

```{r data-analysis, eval=FALSE}
observed_synergies_file = "data/observed_synergies_cascade_2.0"
observed_synergies = emba::get_observed_synergies(observed_synergies_file)

data_dir = "/home/john/tmp/ags_paper_res/link-only/hsa/bootstrap_random_ss"
res_dirs = list.files(data_dir, full.names = TRUE)

data_list = list()
index = 1
for (dir in res_dirs) {
  if (stringr::str_detect(string = dir, pattern = "cascade_2.0_random_hsa_ss")) {
    # How many stable states the models had?
    ss_class = str_match(string = dir, pattern = "_(ss\\d+)_")[1,2]
    
    # Get the ensemble-wise synergies
    ew_syn_file = list.files(path = dir, pattern = "_ensemblewise_synergies.tab", full.names = TRUE)
    ew_synergies = emba::get_synergy_scores(ew_syn_file)
    
    observed = sapply(ew_synergies$perturbation %in% observed_synergies, as.integer)
    
    # ROC AUC (Ensemble-wise)
    ew_roc = roc.curve(scores.class0 = ew_synergies %>% pull(score) %>% (function(x) {-x}), 
      weights.class0 = observed)
    ew_roc_auc = ew_roc$auc
    
    # PR AUC (Ensemble-wise)
    ew_pr = pr.curve(scores.class0 = ew_synergies %>% pull(score) %>% (function(x) {-x}), 
  weights.class0 = observed)
    ew_pr_auc = ew_pr$auc.davis.goadrich
    
    # Get the model-wise synergies
    mw_syn_file = list.files(path = dir, pattern = "_modelwise_synergies.tab", full.names = TRUE)
    mw_synergies = emba::get_synergy_scores(mw_syn_file, file_type = "modelwise")
    
    mw_synergies = mw_synergies %>% 
      mutate(synergy_prob = case_when(
        synergies == 0 & `non-synergies` == 0 ~ 0, 
        TRUE ~ synergies/(synergies + `non-synergies`)))
    
    # ROC AUC (Model-wise)
    mw_roc = roc.curve(scores.class0 = mw_synergies %>% pull(synergy_prob), weights.class0 = observed)
    mw_roc_auc = mw_roc$auc
    
    # PR AUC (Model-wise)
    mw_pr = pr.curve(scores.class0 = mw_synergies %>% pull(synergy_prob), weights.class0 = observed)
    mw_pr_auc = mw_pr$auc.davis.goadrich
    
    # Merge results
    data_list[[index]] = dplyr::bind_cols(ss_class = ss_class, 
      roc_auc = ew_roc_auc, pr_auc = ew_pr_auc, method = "ensemble-wise")
    index = index + 1
    data_list[[index]] = dplyr::bind_cols(ss_class = ss_class, 
      roc_auc = mw_roc_auc, pr_auc = mw_pr_auc, method = "model-wise")
    index = index + 1
  }
}

res = dplyr::bind_rows(data_list)

saveRDS(res, file = "data/res.rds")
```

We will just load the result for efficiency:
```{r load-results}
res = readRDS(file = "data/res.rds")
```

## Ensemble-wise Synergies {-}

```{r data-vis-ew, warning=FALSE, cache=TRUE}
my_comparisons = list(c("ss0", "ss1"), c("ss0", "ss2"), c("ss1", "ss2"), 
  c("ss0", "ss01"), c("ss1", "ss01"), c("ss2", "ss02"), c("ss01", "ss012"),  c("ss12", "ss012"))
ggboxplot(data = res %>% filter(method == "ensemble-wise"), 
  x = "ss_class", y = "roc_auc", fill = "ss_class",
  xlab = "Random Model Group (#Stable States)", ylab = "ROC AUC",
  order = c("ss0", "ss1", "ss2", "ss01", "ss02", "ss12", "ss012")) + 
  stat_compare_means(comparisons = my_comparisons, method = "wilcox.test", label = "p.signif")

ggboxplot(data = res %>% filter(method == "ensemble-wise"),
  x = "ss_class", y = "pr_auc", fill = "ss_class", 
  xlab = "Random Model Group (#Stable States)", ylab = "PR AUC",
  order = c("ss0", "ss1", "ss2", "ss01", "ss02", "ss12", "ss012")) +
  stat_compare_means(comparisons = my_comparisons, method = "wilcox.test", label = "p.signif")
```

- ROC AUC results are somewhat analogous to the PR AUC results
- Getting a **complete random sample of models with 1 stable state** is enough to give you a *valid* ROC AUC (average AUC: `r res %>% filter(method == "ensemble-wise", ss_class == "ss1") %>% summarise(mean(roc_auc))`) - the *ss1* group is one of the highest-performing groups
- Getting a **decent random sample** (keeping true to the models proportions as seeing in the table above) means being close to what the class *ss01* represents.
The class **ss01** is 50-50 and a genuine decent random sample would be ~60-40 (60% models with 0 stable states and 40% models with 1 stable state), but we see that the 0 stable state models do not add *AUC points* (if I am allowed such terminology :) to any group that they are combined with!
- Continuing, the *ss01* group is **indistiguisable performance-wise** from the *ss1* group - meaning that **using just the $1$ stable state models as a random sample for this topology (CASCADE 2.0) is a good enough choice** (so, we did alright)!

## Model-wise Synergies {-}

```{r data-vis-mw, warning=FALSE, cache=TRUE}
ggboxplot(data = res %>% filter(method == "model-wise"),
  x = "ss_class", y = "roc_auc", fill = "ss_class",
  xlab = "Random Model Group (#Stable States)", ylab = "ROC AUC",
  order = c("ss0", "ss1", "ss2", "ss01", "ss02", "ss12", "ss012")) +
  stat_compare_means(comparisons = my_comparisons, method = "wilcox.test", label = "p.signif")

ggboxplot(data = res %>% filter(method == "model-wise"),
  x = "ss_class", y = "pr_auc", fill = "ss_class",
  xlab = "Random Model Group (#Stable States)", ylab = "PR AUC",
  order = c("ss0", "ss1", "ss2", "ss01", "ss02", "ss12", "ss012")) + 
  stat_compare_means(comparisons = my_comparisons, method = "wilcox.test", label = "p.signif")
```

- Same results as above.
Note that the model-wise methodology (used to calculate the synergy scores) seems to be more *granular*: it's easier to distinguish the different categories performance using this methodology.

# R session info {-}

```{r session info, comment=""}
xfun::session_info()
```

# References {-}
