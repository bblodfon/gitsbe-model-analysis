---
title: "Gitsbe Cell Line Model Analysis"
output:
  html_document:
    df_print: paged
    #self_contained: no
---

## Intro

This notebook includes the ensemble model analysis performed on the models 
generated by the `Gitsbe` module when running the DrugLogics computational 
pipeline for finding synergistic drug combinations (drug pairs). All these 
models were trained towards a specific steady state signaling pattern that was
derived based on input data (gene expression, CNV) for a particular cell line. 
The input for the simulations and the output data are in the `cell-lines-2500` 
directory (the 2500 number denotes the number of simulations executed). The 
analysis will be presented step by step in the sections below.

The R version used for this analysis is: **3.4.4 (Someone to Lean On)**.

## Prerequisites

Firstly, we load the required libraries (you need to install them if you don't
have them):
```{r Load libraries, message=FALSE}
# require(RColorBrewer)
# require(heatmap.plus)
# require(gplots)
require(rje)
```
and the relevant helper functions:
```{r Helper functions}
# Set the working directory to the gitsbe-model-analysis folder: 
# setwd("pathTo/gitsbe-model-analysis")
source("Rscripts/input_functions.R")
source("Rscripts/output_functions.R")
source("Rscripts/analysis_functions.R")
source("Rscripts/plot_functions.R")
```

## Input

We will define the name of the cell line which must match the name of the 
directory that has the input files inside the `cell-lines-2500` directory. Our
analysis will be done on the `A498` cell line (though it is applicable with 
small technical changes to the other cell lines):
```{r Input: cell line}
cell.line = "A498"
data.dir = paste0(getwd(), "/cell-lines-2500/", cell.line, "/")

# create 'results' directory if it doesn't exist already 
results.dir = paste0(data.dir, "results")
dir.create(results.dir, showWarnings = FALSE)

print(paste("Cell line:", cell.line))
```
**Three inputs** are used in this analysis:

- The **model_predictions** file which has for each model the prediction for 
each drug combination tested (*0* = no synergy predicted, *1* = synergy 
predicted, *NA* = couldn't find stable states in either the drug combination 
inhibited model or in any of the two single-drug inhibited models)
- The **observed_synergies** file which lists the drug combinations that were 
observed as synergistic for the particular cell line.
- The **models** directory, which is the same as the models directory produced 
by `Gitsbe` and has one `.gitsbe` file per model that includes this info:
    - The *fitness* to the steady state (value between 0 and 1)
    - The *stable state* of the boolean model. Note that a model can have 1 
stable state or none in our simulations - but the models used in this analysis 
have been selected through a genetic evolution algorithm in `Gitsbe` and so in 
the end, only those with 1 stable state have higher fitness values and remain in 
the later generations. Higher fitness here means a better match of a model's 
stable state to the cell line derived steady state (a perfect match would result 
in a fitness of 1)
    - The *boolean equations* of the model

```{r Input: files}
model.predictions.file = paste0(data.dir, "model_predictions")
observed.synergies.file = paste0(data.dir, "observed_synergies")
models.dir = paste0(data.dir, "models")
```
Now, we parse the data into proper R objects. First the synergy predictions 
per model:
```{r Input: model predictions}
model.predictions = get.model.predictions(model.predictions.file)
head(model.predictions)
```
So, we can see that our dataset has the models as rows and each column is a 
different drug combination that was tested in our simulations.
```{r Model&Drug Stats}
drug.combinations.tested = colnames(model.predictions)
models                   = rownames(model.predictions)
nodes                    = get.node.names(models.dir)

number.of.drugs.tested = length(drug.combinations.tested)
number.of.models       = length(models)
number.of.nodes        = length(nodes)

print(paste("Drug combinations tested: ", number.of.drugs.tested, "",
            "Number of models: ", number.of.models, "",
            "Number of nodes:", number.of.nodes))
```
Next, we get the full stable state, the equations and fitness score per model:
```{r Input: models stable states}
models.stable.state = get.stable.state.from.models.dir(models.dir)
models.stable.state = models.stable.state[models,]
head(as.data.frame(models.stable.state))
```
The rows of the above dataset represent the models while the columns are the
names of the nodes (proteins, genes, etc.) of the cancer cell network under
study. So, each model has one stable state which means that in every model, the
nodes in the network have reached a state of either 0 (inhibition) or 1
(activation).
```{r Input: models equations}
models.equations = get.equations.from.models.dir(
  models.dir, remove.equations.with.link.operator = TRUE)
models.equations = models.equations[models,]
head(as.data.frame(models.equations))
```
For the equations, if we look at a specific row (a model so to speak), the 
columns (node names) correspond to the *targets of regulation* (and the network 
has been built so that *every node is a target* - i.e. it has other nodes 
activating or inhibiting it). The general form of a boolean equation is: 
**Target \*= (Activator OR Activator OR...) AND NOT (Inhibitor OR Inhibitor 
OR...)**.

The difference between the models' boolean equations is the *link operator* 
(OR NOT/AND NOT) which has been mutated (changed) through the evolutionary 
process of the genetic algorithm in `Gitsbe`. For example, if a model has for 
the column `ERK_f` a value of 1, the correspoding equation is: 
ERK_f \*= (MEK_f) **OR NOT** ((DUSP6) OR PPP1CA). A value of 0 would correspond 
to the same equation but having **AND NOT**. Note that the equations that do not
have link operators (meaning that they are *the same for every model*) are 
discarded (so less columns in this dataset) since we want to see the 
change/difference between the models.

We also keep track of each model fitness to the steady state (a value closer 
to 1 corresponds to a better fit to the steady state profile):
```{r Input: models fitness}
models.fitness = get.fitness.from.models.dir(models.dir)
models.fitness = models.fitness[models]
head(as.data.frame(models.fitness))
```
Lastly, the synergies observed are:
```{r Input: observed synergies}
observed.synergies = get.observed.synergies(
  observed.synergies.file, drug.combinations.tested)
number.of.observed.synergies = length(observed.synergies)

print(paste("Number of synergies observed:", number.of.observed.synergies))
observed.synergies
```

## Analysis

It will be interesting to know the percentage of the above observed synergies 
that were actually predicted by at least one of the models (there might be 
combinations that no model out of the 7500 could predict):
```{r Synergy stats 1}
# Subset the model.data to the observed synergies
observed.model.predictions = 
  model.predictions[,sapply(drug.combinations.tested, function(drug.comb) {
    is.correct.synergy(drug.comb, observed.synergies)
  })]

# Subset the model.data to the unobserved synergies
unobserved.model.predictions = 
  model.predictions[,sapply(drug.combinations.tested, function(drug.comb) {
    !is.correct.synergy(drug.comb, observed.synergies)
  })]

stopifnot(dim(observed.model.predictions)[2] + 
          dim(unobserved.model.predictions)[2] == number.of.drugs.tested)

number.of.models.per.observed.synergy = 
  apply(observed.model.predictions, 2, sum, na.rm = T)
predicted.synergies = names(number.of.models.per.observed.synergy)[
                            number.of.models.per.observed.synergy > 0]

number.of.models.per.observed.synergy
predicted.synergies
predicted.synergies.percentage = 100 * length(predicted.synergies) /
                    number.of.observed.synergies
print(paste0("Percentage of predicted synergies (by at least one model): ", 
             specify.decimal(predicted.synergies.percentage, 2), "%"))
```
So, for the `A498` cell line, there were indeed synergies that no model could 
predict (only 5 out of 17 were actually found by at least one model). Next, we 
want to know the maximum number of observed synergies predicted by one model 
alone - can one model by itself predict all the 5 synergies or do we need many 
models to capture this diverse synergy landscape? To do that, we go even further 
and **count the number of models that predict a specific set of synergies** for 
every possible combination subset of the predicted synergy set found above:
```{r Synergy stats 2}
# Find the number of predictive models for every synergy subset
predicted.synergies.powerset = powerSet(predicted.synergies)
predicted.synergies.powerset = predicted.synergies.powerset[
  order(sapply(predicted.synergies.powerset, length))
]
names(predicted.synergies.powerset) = 
  sapply(predicted.synergies.powerset, function(drug.comb.set) {
    paste(drug.comb.set, collapse = ",")
  })

synergy.subset.stats = 
  sapply(predicted.synergies.powerset, function(drug.comb.set) {
    count.models.that.predict.synergy.set(drug.comb.set, observed.model.predictions) 
})

# Bar plot of the number of models for every possible observed synergy combination set
# Tweak the threshold.for.subset.removal and bottom.margin as desired
make.barplot.on.synergy.subset.stats(synergy.subset.stats,
                                     threshold.for.subset.removal = 1, 
                                     bottom.margin = 9, cell.line)
```

As can be seen from the figure above (where we excluded sets of synergies that 
were predicted by no model by setting the `threshold.for.subset.removal` value 
to 1), **almost half of the models predict no synergies**, while the `PI-D1` 
synergy is predicted by almost all of the rest of the models. Next we calculate 
the maximum number of correctly predicted synergies (TP - True Positives) per 
model:
```{r TP Stats}
# Count the predictions of the observed synergies per model (TP)
models.synergies.tp = apply(observed.model.predictions, 1, sum, na.rm = T)
models.synergies.tp.stats = table(models.synergies.tp)

# Bar plot of number of models vs correctly predicted synergies
make.barplot.on.model.stats(models.synergies.tp.stats, cell.line)
```

So, there were indeed **only 2 models that predicted 3 synergies** (the set 
`BI-PD,PD-PI,PI-D1`), whereas no model could predict all 5 of the observed 
synergies. The power of the ensemble model approach lies in the fact that (as we
saw from the above figures) we do not have *individual super models* that can 
predict many observed drug combinations, but many that predict *at least one* 
and which will be used by the drug response analysis module (`Drabme`) to 
better infer the synergistic drug combinations. It goes without saying though, 
that the existance of models that could predict more than a handful of synergies 
would be beneficial for any approach that performs drug response analysis on a 
multitude of models.

Now, we want to investigate and find possible important nodes - **biomarkers** -
whose activity state either distinguishes good performance models from less 
performant ones (in terms of maximum true positives predicted) or makes some 
models predict a specific synergy (or set of synergies) compared to others that 
can't (but can predict different ones). So, we devised *two strategies* to split 
the models in our disposal to *good* and *bad* ones (but not necessarily all of 
them), the demarcation line being either a performance metric (number of TP) or 
the prediction or not of a specific synergy (or set of synergies). Then, for 
each group of models (labeled as either *good* or *bad*) we find the average 
activity state of every node in the network (value between 0 and 1) and then we 
compute the average difference for each node: 
$\forall i\in nodes,mean(state_i)_{good} - mean(state_i)_{bad}$. 
Our hypothesis is that if the absolute value of these average differences are 
larger than a user-defined threshold (e.g. 0.7) for a small number of nodes (the 
less the better) while for the rest of the nodes they remain close to zero, then 
the former nodes are considered **the most important** since they 
define the difference between the *average bad model* and the *average good* one. 
We will also use a coloring method to visualize these average differences on the
network nodes.

Using our first strategy, **we will split the models based on the number of true 
positive predictions**. For example, the bad models will be the ones that 
predicted 0 TP synergies whereas the good models will be the ones that predicted 
2 TP (we will denote the grouping as (0,2)). This particular classification 
strategy will be used for every possible combination of the number of TP as 
given by the `models.synergies.tp.stats` object and the density estimation of the
average node state differences in each case will be ploted in a common graph:
```{r Good vs Bad models based on the number of TPs}
tp.values = as.numeric(names(models.synergies.tp.stats))
tp.values.comb = t(combn(tp.values, 2))

diff.tp.results = apply(tp.values.comb, 1, function(comb) {
  return(get.avg.activity.diff.based.on.tp.predictions(
    models, models.synergies.tp, models.stable.state, 
    num.low = comb[1], num.high = comb[2]))
})

tp.comb.names = apply(tp.values.comb, 1, function(row) {
  return(paste0("(", paste(row, collapse = ","), ")")) 
})
colnames(diff.tp.results) = tp.comb.names
diff.tp.results = t(diff.tp.results)

densities = apply(abs(diff.tp.results), 1, density)
make.multiple.density.plot(densities)
```

What we are actually looking for is density plots that are *largely skewed to 
the right* (so the average absolute differences of these nodes are close to zero)
while there are a few areas of non-zero densities which are as close to 1 as 
possible. So, from the above graph, the density plots that fit this description 
are the oens marked as (2,3) and the even better (1,3). Next we will visualize 
this differences in a network graph, where 

- build the network (igraph?), decide layout, arrow heads
- coloring of the nodes represents the diff value: [-1, 0, 1]
- size of node represents the abs(diff) value: [0, 1], where 0: normal size, 1: a max size?





Two synergies of interest in A498 cell line `AK-PD`, `PD-PI`
ways to find out what makes the models predict these synergies with regards to 
their stable state characteristics and boolean equation link operator mutations

```{r eval=FALSE, include=FALSE}
# Good vs Bad models based on the number of the models
# that predicted a specific synergy
predicted.synergies = synergy_observations_data
for (drugComb in synergy_observations_data) {
  diff_specific = get.diff.specific.synergy(drugComb, observed.modelData, models_stable_state)
  if (sum(is.na(diff_specific)) == length((diff_specific))) {
    print(paste("No models predicted the ", drugComb, " synergy", sep = ""))
    predicted.synergies = predicted.synergies[! predicted.synergies %in% drugComb]
  } else {
    #output.diff.to.file(cell_line, drugComb, diff_specific)
  }
}
non.predicted.synergies = synergy_observations_data[!synergy_observations_data %in% predicted.synergies]

# Good vs Bad models when comparing models that predicted diferrent synergy sets
# Usually good models predicted: (A-B, C-D, K-L) and bad (A-B, C-D) - just to see
# what the good models have that allows them to predict the K-L synergy also
diff.on.synergy.sets = 
  get.diff.from.models.predicting.diff.synergy.sets(all.synergy.subsets["AK-D1,BI-D1,PK-ST"], all.synergy.subsets["AK-BI,AK-D1,BI-D1,PK-ST"], observed.modelData, models_stable_state)
```
